[default]
#the loss function to be minimized
loss = average_cross_entropy
#the amount of training labels that need to be added to the output e.g. eos
trainlabels = 1
#a space seperated list of target names used by the trainer
targets = 
#number of passes over the entire database
num_epochs = 100
#initial learning rate of the neural net
initial_learning_rate = 1e-3
#exponential weight decay parameter
learning_rate_decay = 0.1
#size of the minibatch (#utterances)
batch_size = 16
#the norm constraint for the weights of the network, set to None for
#unconstrainted
norm_constraint = None
#you can cut a batch of sequences to equal length parts, this is the length of
#those parts, set to 0 for not cutting the data
cut_sequence_length = 0
#number of minibatches to aggregate before updating the parameters if 0
#asynchronous training will be done
numbatches_to_aggregate = 0
#the data will be divided into buckets according to sequence length, this
#setting determines the number of buckets to use. For no bucketing set to 1
numbuckets = 16
# bool, change batch size from bucket to bucket, for buckets with higher
#seq_length a smaller batch size is used
variable_batch_size = True

###VALIDATION PART###
#frequency of evaluating the validation set.
valid_frequency = 500
#if you want to adapt the learning rate based on the validation set, set to True
valid_adapt = False
#if you want to go back in training if validation performance is worse set to
#True
go_back = False
#the number of times validation performance can be worse before terminating
#training, set to None to disable early stopping
num_tries = 2
#set to True if you want to reset the number of tries if the validation
#performance is better
reset_tries = True
